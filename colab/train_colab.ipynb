{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-1.7B GSM8K PPO Training (VERL)\n",
    "\n",
    "Replicates reported improvement on GSM8K:\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| Qwen3-1.7B base | ~69.2% |\n",
    "| Qwen3-1.7B + PPO | ~82.7% |\n",
    "\n",
    "**GPU recommendation:** A100 40GB (Colab Pro) or T4 16GB (free tier, with reduced batch sizes)\n",
    "\n",
    "**Runtime:** ~6–12 hours on A100 for 500 steps. Use Colab Pro with background execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu.name}  VRAM: {gpu.total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Project Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\"  # <-- UPDATE THIS\n",
    "REPO_DIR = \"/content/LLM_RL\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    !git -C {REPO_DIR} pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VERL\n",
    "!git clone https://github.com/verl-project/verl /content/verl\n",
    "%cd /content/verl\n",
    "!pip install -e . -q\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining dependencies\n",
    "!pip install -q \\\n",
    "    vllm \\\n",
    "    datasets \\\n",
    "    sympy \\\n",
    "    regex \\\n",
    "    pandas \\\n",
    "    pyarrow \\\n",
    "    transformers>=4.44.0 \\\n",
    "    accelerate \\\n",
    "    flash-attn --no-build-isolation\n",
    "\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Login (required to download Qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Get your token from https://huggingface.co/settings/tokens\n",
    "# Use Colab Secrets (key icon on left sidebar) → add HF_TOKEN\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in via Colab secret.\")\n",
    "except Exception:\n",
    "    login()  # interactive prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPO_DIR)\n",
    "!python data/prepare_gsm8k.py --output_dir data/gsm8k\n",
    "!ls data/gsm8k/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rewards/gsm8k_reward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Evaluate Base Model Before Training\n",
    "\n",
    "This gives you the ~69% baseline to compare against after PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model (takes ~30-45 min on T4, ~15 min on A100)\n",
    "# You can skip this and come back after training.\n",
    "RUN_BASE_EVAL = False  # Set to True to run\n",
    "\n",
    "if RUN_BASE_EVAL:\n",
    "    !python evaluation/eval_gsm8k.py \\\n",
    "        --model_path Qwen/Qwen3-1.7B \\\n",
    "        --split test \\\n",
    "        --max_new_tokens 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detect GPU and Configure Training\n",
    "\n",
    "- **A100 (40GB)**: uses full config, batch_size=128\n",
    "- **T4 (16GB)**: uses reduced config, batch_size=32, response_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "print(f\"GPU memory: {gpu_mem_gb:.1f} GB\")\n",
    "\n",
    "IS_A100 = gpu_mem_gb >= 38\n",
    "print(f\"Using {'A100' if IS_A100 else 'T4/low-VRAM'} config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Colab-specific override config\n",
    "import os\n",
    "\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "\n",
    "if IS_A100:\n",
    "    colab_overrides = \"\"\"\n",
    "data:\n",
    "  train_batch_size: 128\n",
    "  max_response_length: 1024\n",
    "\n",
    "actor_rollout_ref:\n",
    "  actor:\n",
    "    ppo_mini_batch_size: 64\n",
    "    ppo_micro_batch_size_per_gpu: 8\n",
    "  rollout:\n",
    "    response_length: 1024\n",
    "    gpu_memory_utilization: 0.45\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "critic:\n",
    "  ppo_micro_batch_size_per_gpu: 8\n",
    "\n",
    "trainer:\n",
    "  n_gpus_per_node: 1\n",
    "  total_epochs: 15\n",
    "  save_freq: 50\n",
    "  test_freq: 25\n",
    "\"\"\"\n",
    "else:\n",
    "    # T4 16GB — reduce everything\n",
    "    colab_overrides = \"\"\"\n",
    "data:\n",
    "  train_batch_size: 32\n",
    "  max_prompt_length: 384\n",
    "  max_response_length: 512\n",
    "\n",
    "actor_rollout_ref:\n",
    "  model:\n",
    "    enable_gradient_checkpointing: True\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "  actor:\n",
    "    ppo_mini_batch_size: 16\n",
    "    ppo_micro_batch_size_per_gpu: 2\n",
    "  rollout:\n",
    "    response_length: 512\n",
    "    gpu_memory_utilization: 0.35\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "critic:\n",
    "  ppo_micro_batch_size_per_gpu: 2\n",
    "  model:\n",
    "    enable_gradient_checkpointing: True\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "trainer:\n",
    "  n_gpus_per_node: 1\n",
    "  total_epochs: 10\n",
    "  save_freq: 50\n",
    "  test_freq: 50\n",
    "\"\"\"\n",
    "\n",
    "with open(\"configs/colab_overrides.yaml\", \"w\") as f:\n",
    "    f.write(colab_overrides)\n",
    "print(\"Config written.\")\n",
    "print(colab_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PPO Training\n",
    "\n",
    "> **Important:** Enable Colab background execution before running:\n",
    "> Runtime → Run all (or just this section), then enable **Background execution** under Runtime → Change runtime type.\n",
    "> This prevents the session from disconnecting during the long training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "os.environ[\"PYTHONPATH\"] = REPO_DIR  # so VERL can find rewards/\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"verl.trainer.main_ppo\",\n",
    "    \"--config-path\", f\"{REPO_DIR}/configs\",\n",
    "    \"--config-name\", \"qwen3_gsm8k_ppo\",\n",
    "    # Override reward function path so VERL finds it\n",
    "    f\"reward_model.reward_fn_path={REPO_DIR}/rewards/gsm8k_reward.py\",\n",
    "    f\"reward_model.reward_fn_name=compute_score\",\n",
    "    # Load colab-specific overrides\n",
    "    \"+trainer.n_gpus_per_node=1\",\n",
    "    f\"trainer.default_local_dir={REPO_DIR}/checkpoints\",\n",
    "]\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\nStarting training... (this will take several hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training — output streams to cell\n",
    "# On Colab Pro, enable background execution to survive disconnects.\n",
    "!python -m verl.trainer.main_ppo \\\n",
    "    --config-path {REPO_DIR}/configs \\\n",
    "    --config-name qwen3_gsm8k_ppo \\\n",
    "    trainer.n_gpus_per_node=1 \\\n",
    "    data.train_files={REPO_DIR}/data/gsm8k/train.parquet \\\n",
    "    data.val_files={REPO_DIR}/data/gsm8k/test.parquet \\\n",
    "    reward_model.reward_fn_path={REPO_DIR}/rewards/gsm8k_reward.py \\\n",
    "    reward_model.reward_fn_name=compute_score \\\n",
    "    trainer.default_local_dir={REPO_DIR}/checkpoints \\\n",
    "    trainer.experiment_name=qwen3_gsm8k_ppo_colab 2>&1 | tee {REPO_DIR}/training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoints = sorted(glob.glob(f\"{REPO_DIR}/checkpoints/qwen3_gsm8k_ppo_colab/global_step_*\"))\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    print(f\"Latest checkpoint: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet.\")\n",
    "    latest_ckpt = \"Qwen/Qwen3-1.7B\"  # fallback to base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPO_DIR)\n",
    "!python evaluation/eval_gsm8k.py \\\n",
    "    --model_path {latest_ckpt} \\\n",
    "    --split test \\\n",
    "    --max_new_tokens 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to Google Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO_DRIVE = False  # Set True to mount Drive and copy results\n",
    "\n",
    "if SAVE_TO_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_DIR = \"/content/drive/MyDrive/LLM_RL_results\"\n",
    "    !mkdir -p {DRIVE_DIR}\n",
    "    !cp -r {REPO_DIR}/checkpoints {DRIVE_DIR}/\n",
    "    !cp -r {REPO_DIR}/evaluation/results {DRIVE_DIR}/\n",
    "    !cp {REPO_DIR}/training.log {DRIVE_DIR}/\n",
    "    print(f\"Saved to {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Print Accuracy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob\n",
    "\n",
    "result_files = glob.glob(f\"{REPO_DIR}/evaluation/results/*.json\")\n",
    "for rf in sorted(result_files):\n",
    "    with open(rf) as f:\n",
    "        data = json.load(f)\n",
    "    r = data[\"report\"]\n",
    "    print(f\"{r['model_path']}  →  {r['accuracy']*100:.2f}%  ({r['correct']}/{r['total']})\")"
   ]
  }
 ]
}
