{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-1.7B GSM8K PPO Training (VERL)\n",
    "\n",
    "Replicates reported improvement on GSM8K:\n",
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| Qwen3-1.7B base | ~69.2% |\n",
    "| Qwen3-1.7B + PPO | ~82.7% |\n",
    "\n",
    "**GPU recommendation:** A100 40GB (Colab Pro) or T4 16GB (free tier, with reduced batch sizes)\n",
    "\n",
    "**Runtime:** ~6–12 hours on A100 for 500 steps. Use Colab Pro with background execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu.name}  VRAM: {gpu.total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive (saves checkpoints + logs — survives disconnects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LLM_RL_runs'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f'Drive mounted. All checkpoints and logs will be saved to: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Project Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/jiayiderekchen/LLM_RL.git\"\n",
    "REPO_DIR = \"/content/LLM_RL\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    !git -C {REPO_DIR} pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VERL\n",
    "!git clone https://github.com/verl-project/verl /content/verl\n",
    "%cd /content/verl\n",
    "!pip install -e . -q\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining dependencies\n",
    "!pip install -q \\\n",
    "    vllm \\\n",
    "    datasets \\\n",
    "    sympy \\\n",
    "    regex \\\n",
    "    pandas \\\n",
    "    pyarrow \\\n",
    "    transformers>=4.44.0 \\\n",
    "    accelerate \\\n",
    "    flash-attn --no-build-isolation\n",
    "\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Login (required to download Qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Get your token from https://huggingface.co/settings/tokens\n",
    "# Use Colab Secrets (key icon on left sidebar) → add HF_TOKEN\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in via Colab secret.\")\n",
    "except Exception:\n",
    "    login()  # interactive prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPO_DIR)\n",
    "!python data/prepare_gsm8k.py --output_dir data/gsm8k\n",
    "!ls data/gsm8k/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rewards/gsm8k_reward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Evaluate Base Model Before Training\n",
    "\n",
    "This gives you the ~69% baseline to compare against after PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model (takes ~30-45 min on T4, ~15 min on A100)\n",
    "# You can skip this and come back after training.\n",
    "RUN_BASE_EVAL = False  # Set to True to run\n",
    "\n",
    "if RUN_BASE_EVAL:\n",
    "    !python evaluation/eval_gsm8k.py \\\n",
    "        --model_path Qwen/Qwen3-1.7B \\\n",
    "        --split test \\\n",
    "        --max_new_tokens 512 \\\n",
    "        --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detect GPU and Configure Training\n",
    "\n",
    "- **A100 (40GB)**: uses full config, batch_size=128\n",
    "- **T4 (16GB)**: uses reduced config, batch_size=32, response_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "print(f\"GPU memory: {gpu_mem_gb:.1f} GB\")\n",
    "\n",
    "IS_A100 = gpu_mem_gb >= 38\n",
    "print(f\"Using {'A100' if IS_A100 else 'T4/low-VRAM'} config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Colab-specific override config\n",
    "import os\n",
    "\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "\n",
    "if IS_A100:\n",
    "    colab_overrides = \"\"\"\n",
    "data:\n",
    "  train_batch_size: 128\n",
    "  max_response_length: 1024\n",
    "\n",
    "actor_rollout_ref:\n",
    "  actor:\n",
    "    ppo_mini_batch_size: 64\n",
    "    ppo_micro_batch_size_per_gpu: 8\n",
    "  rollout:\n",
    "    response_length: 1024\n",
    "    gpu_memory_utilization: 0.45\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "critic:\n",
    "  ppo_micro_batch_size_per_gpu: 8\n",
    "\n",
    "trainer:\n",
    "  n_gpus_per_node: 1\n",
    "  total_epochs: 15\n",
    "  save_freq: 50\n",
    "  test_freq: 25\n",
    "\"\"\"\n",
    "else:\n",
    "    # T4 16GB — reduce everything\n",
    "    colab_overrides = \"\"\"\n",
    "data:\n",
    "  train_batch_size: 32\n",
    "  max_prompt_length: 384\n",
    "  max_response_length: 512\n",
    "\n",
    "actor_rollout_ref:\n",
    "  model:\n",
    "    enable_gradient_checkpointing: True\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "  actor:\n",
    "    ppo_mini_batch_size: 16\n",
    "    ppo_micro_batch_size_per_gpu: 2\n",
    "  rollout:\n",
    "    response_length: 512\n",
    "    gpu_memory_utilization: 0.35\n",
    "  ref:\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "critic:\n",
    "  ppo_micro_batch_size_per_gpu: 2\n",
    "  model:\n",
    "    enable_gradient_checkpointing: True\n",
    "    fsdp_config:\n",
    "      param_offload: True\n",
    "\n",
    "trainer:\n",
    "  n_gpus_per_node: 1\n",
    "  total_epochs: 10\n",
    "  save_freq: 50\n",
    "  test_freq: 50\n",
    "\"\"\"\n",
    "\n",
    "with open(\"configs/colab_overrides.yaml\", \"w\") as f:\n",
    "    f.write(colab_overrides)\n",
    "print(\"Config written.\")\n",
    "print(colab_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. WandB Login (for live training dashboard)\n",
    "\n",
    "Free at [wandb.ai](https://wandb.ai). After training starts, a dashboard URL will print — open it to see live reward, loss, and accuracy curves.\n",
    "\n",
    "Skip this cell if you don't want WandB (training still runs, just no dashboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb\n",
    "\n",
    "import wandb\n",
    "# Add WANDB_API_KEY to Colab Secrets (key icon), or login interactively below\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
    "    print('Logged in via Colab secret.')\n",
    "except Exception:\n",
    "    wandb.login()  # interactive — paste your API key from wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PPO Training\n",
    "\n",
    "> **Important:** Enable Colab background execution before running:\n",
    "> Runtime → Change runtime type → enable **Background execution**.\n",
    "> This prevents the session from disconnecting during the long training run.\n",
    "\n",
    "Progress is visible in two ways:\n",
    "- **WandB dashboard** (live link printed when training starts)\n",
    "- **Log monitor cell below** — run it in parallel to tail the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(REPO_DIR)\n",
    "os.environ['PYTHONPATH'] = REPO_DIR\n",
    "\n",
    "# Run training — custom_reward_function is a valid VERL config key, no + needed\n",
    "!python -m verl.trainer.main_ppo \\\n",
    "    --config-path {REPO_DIR}/configs \\\n",
    "    --config-name qwen3_gsm8k_ppo \\\n",
    "    trainer.n_gpus_per_node=1 \\\n",
    "    data.train_files={REPO_DIR}/data/gsm8k/train.parquet \\\n",
    "    data.val_files={REPO_DIR}/data/gsm8k/test.parquet \\\n",
    "    +custom_reward_function.path={REPO_DIR}/rewards/gsm8k_reward.py \\\n",
    "    +custom_reward_function.name=compute_score \\\n",
    "    trainer.default_local_dir={REPO_DIR}/checkpoints \\\n",
    "    trainer.experiment_name=qwen3_gsm8k_ppo_colab \\\n",
    "    trainer.project_name=gsm8k_ppo 2>&1 | tee {REPO_DIR}/training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. (Optional) Live Log Monitor\n",
    "Run this in a **separate cell** while training is running to tail key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "LOG_FILE = f\"{REPO_DIR}/training.log\"\n",
    "POLL_INTERVAL = 30  # seconds between refreshes\n",
    "\n",
    "step_pattern = re.compile(r'global_step[:\\s]+(\\d+)')\n",
    "reward_pattern = re.compile(r'reward[:\\s/]+([\\d\\.]+)', re.IGNORECASE)\n",
    "loss_pattern = re.compile(r'(?:actor_loss|loss)[:\\s]+([\\d\\.\\-]+)', re.IGNORECASE)\n",
    "\n",
    "print(f\"Monitoring {LOG_FILE} — refreshes every {POLL_INTERVAL}s. Interrupt to stop.\")\n",
    "\n",
    "history = []  # (step, reward)\n",
    "\n",
    "while True:\n",
    "    if not os.path.exists(LOG_FILE):\n",
    "        print(\"Waiting for training.log to appear...\")\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "\n",
    "    with open(LOG_FILE) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse last 200 lines for metrics\n",
    "    recent = lines[-200:]\n",
    "    steps = [int(m.group(1)) for l in recent if (m := step_pattern.search(l))]\n",
    "    rewards = [float(m.group(1)) for l in recent if (m := reward_pattern.search(l))]\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"=== Training Monitor  [{time.strftime('%H:%M:%S')}] ===\")\n",
    "    print(f\"Log lines so far : {len(lines)}\")\n",
    "    print(f\"Latest step      : {steps[-1] if steps else 'N/A'}\")\n",
    "    print(f\"Latest reward    : {rewards[-1]:.4f}\" if rewards else \"Latest reward    : N/A\")\n",
    "    print(f\"Mean reward (recent 10): {sum(rewards[-10:])/len(rewards[-10:]):.4f}\" if len(rewards) >= 2 else \"\")\n",
    "    print()\n",
    "    print(\"--- Last 10 log lines ---\")\n",
    "    for l in lines[-10:]:\n",
    "        print(l.rstrip())\n",
    "\n",
    "    time.sleep(POLL_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoints = sorted(glob.glob(f\"{REPO_DIR}/checkpoints/qwen3_gsm8k_ppo_colab/global_step_*\"))\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    print(f\"Latest checkpoint: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet.\")\n",
    "    latest_ckpt = \"Qwen/Qwen3-1.7B\"  # fallback to base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPO_DIR)\n",
    "!python evaluation/eval_gsm8k.py \\\n",
    "    --model_path {latest_ckpt} \\\n",
    "    --split test \\\n",
    "    --max_new_tokens 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to Google Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO_DRIVE = False  # Set True to mount Drive and copy results\n",
    "\n",
    "if SAVE_TO_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_DIR = \"/content/drive/MyDrive/LLM_RL_results\"\n",
    "    !mkdir -p {DRIVE_DIR}\n",
    "    !cp -r {REPO_DIR}/checkpoints {DRIVE_DIR}/\n",
    "    !cp -r {REPO_DIR}/evaluation/results {DRIVE_DIR}/\n",
    "    !cp {REPO_DIR}/training.log {DRIVE_DIR}/\n",
    "    print(f\"Saved to {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Print Accuracy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob\n",
    "\n",
    "result_files = glob.glob(f\"{REPO_DIR}/evaluation/results/*.json\")\n",
    "for rf in sorted(result_files):\n",
    "    with open(rf) as f:\n",
    "        data = json.load(f)\n",
    "    r = data[\"report\"]\n",
    "    print(f\"{r['model_path']}  →  {r['accuracy']*100:.2f}%  ({r['correct']}/{r['total']})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
